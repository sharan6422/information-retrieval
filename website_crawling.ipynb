{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu1N2f/oR16ZR3709PkOyf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharan6422/information-retrieval/blob/main/website_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HKhMLoJHdYF"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import string\n",
        "import json\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all the nltk libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr9puahCIXV2",
        "outputId": "41f6f3ab-8af3-4f73-f2ab-a0e7f0c3285c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#other libraries\n",
        "import csv\n",
        "from urllib.parse import urljoin"
      ],
      "metadata": {
        "id": "madn0ZcOImmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web Scrapping and Crawling\n"
      ],
      "metadata": {
        "id": "wTQTbga1IuIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Robot.txt and crawl delay\n",
        "def fetch_crawl_delay(url):\n",
        "    robots_url = urljoin(url, \"/robots.txt\")\n",
        "    response = requests.get(robots_url)\n",
        "    if response.status_code == 200:\n",
        "        robots_content = response.text\n",
        "        for line in robots_content.split('\\n'):\n",
        "            if line.startswith(\"Crawl-delay:\"):\n",
        "                delay = float(line.split(\":\")[1].strip())\n",
        "                return delay\n",
        "    return None"
      ],
      "metadata": {
        "id": "yAVOG3l2Iq6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetching publication details\n",
        "def fetch_publication_details(url, crawl_delay):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    page = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    publications = []\n",
        "\n",
        "    for publication in soup.select('li.list-result-item'):\n",
        "        title_element = publication.select_one('h3.title > a')\n",
        "        title = title_element.get_text(strip=True) if title_element else \"Title Not Found\"\n",
        "        publication_url = title_element['href'] if title_element else \"Publication URL Not Found\"\n",
        "\n",
        "        author_elements = publication.select('a.link.person')\n",
        "        authors = [author.get_text(strip=True) for author in author_elements]\n",
        "        author_links = [author['href'] for author in author_elements]\n",
        "\n",
        "        publication_year_element = publication.select_one('span.date')\n",
        "        publication_year = publication_year_element.get_text(strip=True) if publication_year_element else \"Publication Year Not Found\"\n",
        "\n",
        "        publications.append((title, authors, publication_year, publication_url, author_links))\n",
        "\n",
        "    # being polite\n",
        "    if crawl_delay:\n",
        "        time.sleep(crawl_delay)\n",
        "\n",
        "    return publications"
      ],
      "metadata": {
        "id": "bpA_2aPgI4Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving to csv\n",
        "def save_to_csv(publications, csv_file):\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Title', 'Authors', 'Publication_Year', 'Publication_URL', 'Author_URLs'])\n",
        "        for title, authors, publication_year, publication_url, author_links in publications:\n",
        "            writer.writerow([title, \", \".join(authors), publication_year, publication_url, \", \".join(author_links)])"
      ],
      "metadata": {
        "id": "2vxyTYwBI-Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crawling and saving to csv\n",
        "def main_crawl():\n",
        "    url = \"https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/\"\n",
        "    csv_file = 'publications_and_hyperlinks.csv'\n",
        "\n",
        "    # Check robot.txt\n",
        "    crawl_delay = fetch_crawl_delay(url)\n",
        "    if crawl_delay:\n",
        "        print(\"delay=\", crawl_delay, \"sec\")\n",
        "\n",
        "    # empty list for publication\n",
        "    all_publications = []\n",
        "\n",
        "    # variables for pagination\n",
        "    current_page = 1\n",
        "    count = 0\n",
        "\n",
        "    # dictionary for staff and publications\n",
        "    staff_publications = {}\n",
        "\n",
        "    # Crawl and extract with delay\n",
        "    while True:\n",
        "        result_temp = fetch_publication_details(url + f\"?page={current_page-1}\", crawl_delay)\n",
        "\n",
        "        if not result_temp:\n",
        "            print('Pages end here!!')\n",
        "            break\n",
        "\n",
        "        if result_temp[0][0] == \"Title is not available\" and not result_temp[0][1]:\n",
        "            break\n",
        "        else:\n",
        "            all_publications.extend(result_temp)\n",
        "            print(f\"Scraped {len(result_temp)} publications from {url}?page={current_page}\")\n",
        "            count += len(result_temp)\n",
        "            current_page += 1\n",
        "\n",
        "            # Update dictionary\n",
        "            for _, authors, _, _, _ in result_temp:\n",
        "                for author in authors:\n",
        "                    if author not in staff_publications:\n",
        "                        staff_publications[author] = []\n",
        "                    staff_publications[author].append(_)\n",
        "\n",
        "    print(\"Total publications in numbers:\", count)\n",
        "\n",
        "    # Calculate the number of staff\n",
        "    num_staff = len(staff_publications)\n",
        "\n",
        "    # Calculate the max No:of distinct publications per staff\n",
        "    max_publications = max(len(publications) for publications in staff_publications.values())\n",
        "\n",
        "    print(\"Number of staff whose publications are crawled (approximately):\", num_staff)\n",
        "    print(\"Num of distinct publications per staff:\", max_publications)\n",
        "    # Save  to a CSV\n",
        "    save_to_csv(all_publications, csv_file)\n",
        "    print(\"CSV files are saved as\", csv_file)\n",
        "\n",
        "    return len(all_publications)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_records_crawl = main_crawl()\n",
        "    print(f\"{num_records_crawl} total scrapped\")\n",
        "\n",
        "\n",
        "initial_db = pd.read_csv('publications_and_hyperlinks.csv').rename(columns={'Unnamed: 0':'SN'})\n",
        "initial_db\n",
        "print(f'{initial_db.shape[0]} records scraped')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F17pCezTJBCW",
        "outputId": "b17fd0ab-eef7-4626-a2c3-b15c8f298086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=1\n",
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=2\n",
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=3\n",
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=4\n",
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=5\n",
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=6\n",
            "Scraped 50 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=7\n",
            "Scraped 2 publications from https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=8\n",
            "Pages end here!!\n",
            "Total publications in numbers: 352\n",
            "Number of staff whose publications are crawled (approximately): 69\n",
            "Num of distinct publications per staff: 52\n",
            "CSV files are saved as publications_and_hyperlinks.csv\n",
            "352 total scrapped\n",
            "352 records scraped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read CSV into initial_db and rename the column to 'SN'\n",
        "initial_db = pd.read_csv('publications_and_hyperlinks.csv').rename(columns={'Unnamed: 0':'SN'})\n",
        "\n",
        "# Add 'SN' column\n",
        "initial_db['SN'] = range(len(initial_db))\n",
        "\n",
        "# Save the updated DataFrame\n",
        "initial_db.to_csv('publications_and_hyperlinks.csv', index=False)\n",
        "\n",
        "# Display\n",
        "initial_db\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "3i_Qupm9JIZy",
        "outputId": "0f199dff-6088-465a-d58b-bfc1ff3f4f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Title                Authors  \\\n",
              "0    A bibliometric analysis of shadow education in...            Karakus, M.   \n",
              "1    Able or disabled: why should neurodiverse stud...             Ayoubi, R.   \n",
              "2    Analysing the impact of post-pandemic factors ...             Ayoubi, R.   \n",
              "3    A Review of the Work of the Positive Youth Fou...  Morini, L., Price, C.   \n",
              "4    BAAL–Cambridge University Press Seminar 2024 –...       Orsini-Jones, M.   \n",
              "..                                                 ...                    ...   \n",
              "347  Eǧitim örgütlerinde entelektüel sermayenin yön...            Karakus, M.   \n",
              "348  Practising cultural sensitivity in virtual spaces           Wimpenny, K.   \n",
              "349  Gender mainstreaming or just more male-streaming?                    NaN   \n",
              "350  The charismatic school leader:  Potent myth or...           Crawford, M.   \n",
              "351  Strategic and resource management in primary s...           Crawford, M.   \n",
              "\n",
              "    Publication_Year                                    Publication_URL  \\\n",
              "0           Jul 2024  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "1       25 Sept 2024  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "2        13 Mar 2024  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "3         7 Nov 2024  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "4        22 Nov 2024  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "..               ...                                                ...   \n",
              "347             2008  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "348     30 Sept 2006  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "349             2005  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "350       1 Dec 2002  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "351       1 Dec 1997  https://pureportal.coventry.ac.uk/en/publicati...   \n",
              "\n",
              "                                           Author_URLs   SN  \n",
              "0    https://pureportal.coventry.ac.uk/en/persons/m...    0  \n",
              "1    https://pureportal.coventry.ac.uk/en/persons/r...    1  \n",
              "2    https://pureportal.coventry.ac.uk/en/persons/r...    2  \n",
              "3    https://pureportal.coventry.ac.uk/en/persons/l...    3  \n",
              "4    https://pureportal.coventry.ac.uk/en/persons/m...    4  \n",
              "..                                                 ...  ...  \n",
              "347  https://pureportal.coventry.ac.uk/en/persons/m...  347  \n",
              "348  https://pureportal.coventry.ac.uk/en/persons/k...  348  \n",
              "349                                                NaN  349  \n",
              "350  https://pureportal.coventry.ac.uk/en/persons/m...  350  \n",
              "351  https://pureportal.coventry.ac.uk/en/persons/m...  351  \n",
              "\n",
              "[352 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85f56d21-a8d2-43e7-af30-a3d0c1a7ff97\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Publication_Year</th>\n",
              "      <th>Publication_URL</th>\n",
              "      <th>Author_URLs</th>\n",
              "      <th>SN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A bibliometric analysis of shadow education in...</td>\n",
              "      <td>Karakus, M.</td>\n",
              "      <td>Jul 2024</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Able or disabled: why should neurodiverse stud...</td>\n",
              "      <td>Ayoubi, R.</td>\n",
              "      <td>25 Sept 2024</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/r...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Analysing the impact of post-pandemic factors ...</td>\n",
              "      <td>Ayoubi, R.</td>\n",
              "      <td>13 Mar 2024</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/r...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Review of the Work of the Positive Youth Fou...</td>\n",
              "      <td>Morini, L., Price, C.</td>\n",
              "      <td>7 Nov 2024</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/l...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BAAL–Cambridge University Press Seminar 2024 –...</td>\n",
              "      <td>Orsini-Jones, M.</td>\n",
              "      <td>22 Nov 2024</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/m...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>Eǧitim örgütlerinde entelektüel sermayenin yön...</td>\n",
              "      <td>Karakus, M.</td>\n",
              "      <td>2008</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/m...</td>\n",
              "      <td>347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>Practising cultural sensitivity in virtual spaces</td>\n",
              "      <td>Wimpenny, K.</td>\n",
              "      <td>30 Sept 2006</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/k...</td>\n",
              "      <td>348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>Gender mainstreaming or just more male-streaming?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2005</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>The charismatic school leader:  Potent myth or...</td>\n",
              "      <td>Crawford, M.</td>\n",
              "      <td>1 Dec 2002</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/m...</td>\n",
              "      <td>350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>Strategic and resource management in primary s...</td>\n",
              "      <td>Crawford, M.</td>\n",
              "      <td>1 Dec 1997</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
              "      <td>https://pureportal.coventry.ac.uk/en/persons/m...</td>\n",
              "      <td>351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>352 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85f56d21-a8d2-43e7-af30-a3d0c1a7ff97')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85f56d21-a8d2-43e7-af30-a3d0c1a7ff97 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85f56d21-a8d2-43e7-af30-a3d0c1a7ff97');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e2407bcc-6dfb-4e54-8d30-494663a94147\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e2407bcc-6dfb-4e54-8d30-494663a94147')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e2407bcc-6dfb-4e54-8d30-494663a94147 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_da880bcb-1013-4e56-8513-9ee728b9234c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('initial_db')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_da880bcb-1013-4e56-8513-9ee728b9234c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('initial_db');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "initial_db",
              "summary": "{\n  \"name\": \"initial_db\",\n  \"rows\": 352,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 350,\n        \"samples\": [\n          \"Education as Bulwark of Uselessness.\",\n          \"Participatory action research: An integrated approach towards practice development\",\n          \"Mentoring for attainment: Mentoring for attainment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 88,\n        \"samples\": [\n          \"Aslam, F., Bagiya, Y.\",\n          \"Karakus, M.\",\n          \"Dang, Q. A., Morini, L.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publication_Year\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 279,\n        \"samples\": [\n          \"30 Jul 2024\",\n          \"19 Jan 2021\",\n          \"20 Dec 2019\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Publication_URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 352,\n        \"samples\": [\n          \"https://pureportal.coventry.ac.uk/en/publications/can-the-benefits-of-creative-methods-support-the-inclusion-of-chi\",\n          \"https://pureportal.coventry.ac.uk/en/publications/investigating-cultures-of-equality\",\n          \"https://pureportal.coventry.ac.uk/en/publications/factors-affecting-multi-stakeholders-perspectives-towards-inclusi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Author_URLs\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 82,\n        \"samples\": [\n          \"https://pureportal.coventry.ac.uk/en/persons/katherine-wimpenny, https://pureportal.coventry.ac.uk/en/persons/dimitar-angelov\",\n          \"https://pureportal.coventry.ac.uk/en/persons/mehmet-karakus\",\n          \"https://pureportal.coventry.ac.uk/en/persons/marina-orsini-jones, https://pureportal.coventry.ac.uk/en/persons/yu-hua-chen-chen, https://pureportal.coventry.ac.uk/en/persons/preeti-suri\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 101,\n        \"min\": 0,\n        \"max\": 351,\n        \"num_unique_values\": 352,\n        \"samples\": [\n          228,\n          116,\n          55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "#indexing\n",
        "# Reset the index\n",
        "scraped_initial_db = initial_db.reset_index(drop=True)\n",
        "\n",
        "# Display few rows of the DataFrame\n",
        "scraped_initial_db.head()\n",
        "\n",
        "# Extract Title column initial_db\n",
        "ids = initial_db[\"Title\"]\n",
        "\n",
        "# Find duplicate rows\n",
        "initial_db[ids.isin(ids[ids.duplicated()])]\n",
        "\n",
        "# Extract the row at index 1\n",
        "one_row = initial_db.loc[1, :].copy()\n",
        "\n",
        "# Display the DataFrame\n",
        "one_row\n",
        "\n",
        "def transform_text(text):\n",
        "    #  lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #  punctuation marks\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Lemmatize and stop words\n",
        "    text = lemmatize(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def map_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    hash_tag = {\"V\": wordnet.VERB, \"R\": wordnet.ADV, \"N\": wordnet.NOUN, \"J\": wordnet.ADJ}\n",
        "    return hash_tag.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Lemmatize and remove stop words\n",
        "    lemmatized_text = \"\"\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            lemmatized_text += lemmatizer.lemmatize(token, map_pos(token)) + \" \"\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "#  copy of craped DataFrame\n",
        "processed_initial_db = scraped_initial_db.copy()\n",
        "\n",
        "def transform_df(dataframe):\n",
        "\n",
        "    # Preprocess the Title\n",
        "    dataframe['Title'] = dataframe['Title'].apply(transform_text)\n",
        "\n",
        "    # Convert Authors to lowercase\n",
        "    dataframe['Authors'] = dataframe['Authors'].str.lower()\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    dataframe = dataframe.drop(columns=['Authors', 'Publication_Year'], axis=1)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "# Preprocess\n",
        "transform_df(processed_initial_db)\n",
        "\n",
        "# Display\n",
        "processed_initial_db.head()\n",
        "\n",
        "# Create a copy of DataFrame\n",
        "single = processed_initial_db.loc[0, :].copy()\n",
        "\n",
        "# Print the single row to inspect its content\n",
        "print(single)\n",
        "\n",
        "# Initialize dictionary to store index\n",
        "indexing_trial = {}\n",
        "\n",
        "# Split 'Title' into words\n",
        "words = single.Title.split()\n",
        "\n",
        "# Get 'SN' of single row\n",
        "SN = single.SN\n",
        "\n",
        "# Extract the first word Title\n",
        "word = words[0]\n",
        "\n",
        "# Create a dictionary word key and SN value\n",
        "example = {word: [SN]}\n",
        "\n",
        "\n",
        "def index_documents_for_row(row_data, word_index):\n",
        "    # Split the Title\n",
        "    words = row_data['Title'].split()\n",
        "\n",
        "    # Get the SN of row\n",
        "    index_sn = int(row_data['SN'])\n",
        "\n",
        "    # Iterate through Title\n",
        "    for word in words:\n",
        "        # If the word is already in the word index, update the index entry\n",
        "        if word in word_index.keys():\n",
        "            # if the 'SN' is not there, then add\n",
        "            if index_sn not in word_index[word]:\n",
        "                word_index[word].append(index_sn)\n",
        "        # If word not in word index, create new entry with  'SN'\n",
        "        else:\n",
        "            word_index[word] = [index_sn]\n",
        "\n",
        "    return word_index\n",
        "\n",
        "# Initialize empty dictionary to store  index\n",
        "word_index = {}\n",
        "\n",
        "# Apply indexing 'single' row using the 'word_index' dictionary\n",
        "word_index = index_documents_for_row(row_data=single, word_index={})\n",
        "\n",
        "# Display\n",
        "print(word_index)\n",
        "\n",
        "\n",
        "def full_index_data_frame(df, index):\n",
        "    # Iterate through  DataFrame\n",
        "    for x in range(len(df)):\n",
        "        # current row data\n",
        "        row_data = df.loc[x, :]\n",
        "\n",
        "        # Update the word index using the current row data\n",
        "        index = index_documents_for_row(row_data=row_data, word_index=index)\n",
        "\n",
        "    return index\n",
        "\n",
        "def construct_index(df, index):\n",
        "    # Preprocess\n",
        "    processed_df = transform_df(df)\n",
        "\n",
        "    # full indexing to preprocessed DataFrame\n",
        "    index = full_index_data_frame(df=processed_df, index=index)\n",
        "\n",
        "    return index\n",
        "\n",
        "# Construct word index for processed_initial_db DataFrame\n",
        "indexed = full_index_data_frame(df=processed_initial_db, index={})\n",
        "\n",
        "# Construct the word index for the 'scraped_db' DataFrame\n",
        "indexes = construct_index(df=scraped_initial_db, index={})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpQ_jAlmJVTU",
        "outputId": "de3dd242-b455-4a3d-a2be-2468eb94e2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title               bibliometric analysis shadow education asia pr...\n",
            "Authors                                                   karakus, m.\n",
            "Publication_Year                                             Jul 2024\n",
            "Publication_URL     https://pureportal.coventry.ac.uk/en/publicati...\n",
            "Author_URLs         https://pureportal.coventry.ac.uk/en/persons/m...\n",
            "SN                                                                  0\n",
            "Name: 0, dtype: object\n",
            "{'bibliometric': [0], 'analysis': [0], 'shadow': [0], 'education': [0], 'asia': [0], 'private': [0], 'supplementary': [0], 'tutor': [0], 'implication': [0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def save_index_to_json(index, file_path):\n",
        "    with open(file_path, 'w') as new_file:\n",
        "        json.dump(index, new_file, sort_keys=True, indent=4)\n",
        "\n",
        "def load_index_from_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def update_index(df, file_path):\n",
        "    if len(df) > 0:\n",
        "        prior_index = load_index_from_json(file_path)\n",
        "        new_index = full_index_data_frame(df=df, index=prior_index)\n",
        "        save_index_to_json(new_index, file_path)\n",
        "\n",
        "\n",
        "# Save the word index to 'indexes.json'\n",
        "save_index_to_json(indexes, 'indexes.json')\n",
        "\n",
        "# Load the word index from 'indexes.json'\n",
        "loaded_indexes = load_index_from_json('indexes.json')\n",
        "\n",
        "# Update the word index with new data from 'processed_db' and save it to 'indexes.json'\n",
        "update_index(df=processed_initial_db, file_path='indexes.json')\n",
        "\n",
        "\n",
        "print(len(loaded_indexes))\n",
        "\n",
        "loaded_indexes\n",
        "\n",
        "def preprocess_query(query):\n",
        "    #  lowercase\n",
        "    query = query.lower()\n",
        "\n",
        "    # punctuation marks\n",
        "    query = query.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Lemmatize\n",
        "    query = lemmatize(query)\n",
        "\n",
        "    return query\n",
        "\n",
        "def demonstrate_query_processing():\n",
        "    user_input = input('Enter query: ')\n",
        "    processed_query = preprocess_query(user_input)\n",
        "    print(f'Processed  Query: {processed_query}')\n",
        "    return processed_query\n",
        "\n",
        "demonstrate_query_processing()\n",
        "\n",
        "\n",
        "def preprocess_and_split_query(terms):\n",
        "    preprocessed_query = preprocess_query(terms)\n",
        "    individual_words = preprocessed_query.split()\n",
        "    return individual_words\n",
        "\n",
        "def demonstrate_query_processing():\n",
        "    user_input = input('Enter Query: ')\n",
        "    processed_query = preprocess_query(user_input)\n",
        "    print(f'Processed Query: {processed_query}')\n",
        "    return processed_query\n",
        "\n",
        "# Get the preprocessed and split query\n",
        "dqp = demonstrate_query_processing()\n",
        "split_query_result = preprocess_and_split_query(dqp)\n",
        "\n",
        "print(f'Split Query: {split_query_result}')\n",
        "\n",
        "def get_union(lists):\n",
        "    union = list(set.union(*map(set, lists)))\n",
        "    union.sort()\n",
        "    return union\n",
        "\n",
        "def get_intersection(lists):\n",
        "    intersect = list(set.intersection(*map(set, lists)))\n",
        "    intersect.sort()\n",
        "    return intersect\n",
        "\n",
        "def vertical_search_engine(data_frame, query, word_index):\n",
        "    query_terms = preprocess_and_split_query(query)  # Split\n",
        "    retrieved_sns = []\n",
        "\n",
        "    # Retrieve SNs from the word index\n",
        "    for term in query_terms:\n",
        "        if term in word_index:\n",
        "            retrieved_sns.append(word_index[term])\n",
        "\n",
        "    # Perform Ranked Retrieval if matched\n",
        "    if len(retrieved_sns) > 0:\n",
        "        high_rank_result = get_intersection(retrieved_sns)  # High-rank result is intersection SNs\n",
        "        low_rank_result = get_union(retrieved_sns)  # Low-rank result is union of retrieved SNs\n",
        "        uncommon_sns = [x for x in low_rank_result if x not in high_rank_result]\n",
        "        high_rank_result.extend(uncommon_sns)\n",
        "        result_sns = high_rank_result\n",
        "\n",
        "        # Extract final output containing the search result\n",
        "        final_output = data_frame[data_frame.SN.isin(result_sns)].reset_index(drop=True)\n",
        "\n",
        "        # Merge result DataFrame with the SNs to maintain the order of Intersection ----> Union\n",
        "        dummy = pd.Series(result_sns, name='SN').to_frame()\n",
        "        result_df = pd.merge(dummy, final_output, on='SN', how='left')\n",
        "    else:\n",
        "        result_df = 'No results'\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def test_search_engine(data_frame, word_index):\n",
        "    query = input(\"Enter query: \")\n",
        "    result = vertical_search_engine(data_frame, query, word_index)\n",
        "    return result\n",
        "def final_engine(results):\n",
        "    if isinstance(results, pd.DataFrame):\n",
        "        for i in range(len(results)):\n",
        "            printout = results.loc[i, :]\n",
        "            print(f\"Title: {printout['Title']}\")\n",
        "            print(f\"Author: {printout['Authors']}\")\n",
        "            print(f\"Published: {printout['Publication_Year']}\")\n",
        "            print(f\"Link: {printout['Publication_URL']}\")\n",
        "            print(f\"Author Link: {printout['Author_URLs']}\")\n",
        "            print('')\n",
        "    else:\n",
        "        print(results)\n",
        "def test_search_engine(df, index):\n",
        "    query = input(\"Enter your query: \")\n",
        "    return vertical_search_engine(df, query, index)\n",
        "\n",
        "# Test\n",
        "results = test_search_engine(scraped_initial_db, indexed)\n",
        "final_engine(results)\n",
        "# initial\n",
        "days = 0\n",
        "interval = 7\n",
        "\n",
        "while True:\n",
        "    fetch_publication_details(\"https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/\", interval)\n",
        "    print(f\"Crawled at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f'Next crawl scheduled after {interval} days')\n",
        "    time.sleep(interval * 24 * 60 * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Sz1oBQe-KPs_",
        "outputId": "6ff9fee7-adea-4b43-a1be-6bf6d19104e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1272\n",
            "Enter query: cancer\n",
            "Processed  Query: cancer \n",
            "Enter Query: cancer\n",
            "Processed Query: cancer \n",
            "Split Query: ['cancer']\n",
            "Enter your query: cancer\n",
            "No results\n",
            "Crawled at 2025-01-15 13:33:02\n",
            "Next crawl scheduled after 7 days\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1c9af33b3bd5>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Crawled at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Next crawl scheduled after {interval} days'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WysddfHKCL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}